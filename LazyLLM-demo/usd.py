import os
import json
import re
import argparse
from typing import List
from PyPDF2 import PdfReader
from docx import Document
from lazyllm import ReactAgent, fc_register, LOG, OnlineChatModule, WebModule

# -------------------------- 1. æ ¸å¿ƒé…ç½®ï¼ˆæ–°å¢æ™ºèƒ½æå–è§„åˆ™ï¼‰ --------------------------
PAPER_DIR = "DOCS"
SUPPORTED_FORMATS = [".pdf", ".docx", ".doc", ".txt"]
TITLE_PRIORITY_RULE = "æ ‡é¢˜æå–ä¼˜å…ˆçº§ï¼š1. PDFé¦–é¡µé¡¶éƒ¨/å±…ä¸­æ–‡å­—ï¼›2. æ‘˜è¦ section ä¸Šæ–¹æ–‡å­—ï¼›3. æ­£æ–‡ç¬¬ä¸€éƒ¨åˆ†æ ‡é¢˜ï¼›4. æ–‡ä»¶åï¼ˆä»…ä½œä¸ºæœ€å fallbackï¼‰"

# ä¼ªä»£ç é…ç½®ï¼ˆä»…YOLOç”Ÿæ•ˆï¼‰
PSEUDO_CODE_CONFIG = {
    "target_framework": "PyTorch",
    "code_style": "ç®€æ´ä¼ªä»£ç ï¼ˆä¿ç•™__init__/forwardæ ¸å¿ƒæ–¹æ³•ï¼‰",
    "key_requirement": "ä¸¥æ ¼å¯¹åº”YOLOåˆ›æ–°ç‚¹ï¼ˆbackbone/neck/headï¼‰"
}

# éYOLOè®ºæ–‡å…³é”®è¯åº“ï¼ˆç”¨äºå­—æ®µæ™ºèƒ½æå–ï¼‰
FIELD_EXTRACTION_KEYWORDS = {
    "innovation": ["innovation", "novel", "propose", "proposed", "breakthrough", "improvement"],
    "math": ["formula", "equation", "derivation", "mathematical", "algorithm", "loss function"],
    "dataset": ["dataset", "data set", "corpus", "benchmark", "training data", "test data"],
    "environment": ["python", "pytorch", "tensorflow", "version", "dependency", "library"],
    "hardware": ["gpu", "tpu", "a100", "v100", "rtx", "memory", "ram"],
    "code": ["github", "gitlab", "repository", "open source", "code available", "link:"],
    "comparison": ["compare", "comparison", "baseline", "method", "state-of-the-art", "sota"],
    "metric": ["metric", "accuracy", "perplexity", "bleu", "map", "f1", "precision", "recall"],
    "result": ["result", "performance", "score", "better than", "higher than", "lower than"],
    "conclusion": ["conclusion", "find", "finding", "conclude", "show that", "demonstrate"]
}

CORE_FIELDS = {
    "paper_title": f"è®ºæ–‡å®Œæ•´æ ‡é¢˜ï¼ˆä¸¥æ ¼æŒ‰{TITLE_PRIORITY_RULE}æå–ï¼Œç¦æ­¢ç¼–é€ ï¼‰",
    "is_yolo_related": "æ˜¯/å¦ï¼ˆåŸºäºæ ‡é¢˜+å†…å®¹å…³é”®è¯ï¼‰",
    "innovation_point": {
        "core_innovation": "æ ¸å¿ƒåˆ›æ–°ï¼ˆå¦‚æ¨¡å‹/ç®—æ³•/å®éªŒè®¾è®¡ï¼Œ150å­—å†…ï¼‰",
        "innovation_value": "åˆ›æ–°ä»·å€¼ï¼ˆè§£å†³çš„é—®é¢˜/å­¦æœ¯/åº”ç”¨ä»·å€¼ï¼Œ100å­—å†…ï¼‰",
        "pseudo_code": "éYOLOç³»åˆ—è®ºæ–‡ï¼Œæ— éœ€ç”Ÿæˆä¼ªä»£ç "
    },
    "math_derivation": {
        "key_formulas": "æ ¸å¿ƒå…¬å¼ï¼ˆLaTeXæ ¼å¼ï¼Œå¦‚$L = \\frac{1}{N}\\sum (y - \\hat{y})^2$ï¼‰",
        "derivation_steps": "æ¨å¯¼æ­¥éª¤ï¼ˆåˆ†ç‚¹ï¼šå‡è®¾â†’å˜æ¢â†’ç»“è®ºï¼‰",
        "math_advantage": "æ•°å­¦ä¼˜åŠ¿ï¼ˆå¯¹æ¯”ä¼ ç»Ÿæ–¹æ³•ï¼Œå¦‚è®¡ç®—é‡é™ä½ï¼‰"
    },
    "reproduction_steps": {
        "data_prep": "æ•°æ®é›†ï¼ˆåç§°+è·å–åœ°å€+é¢„å¤„ç†ï¼‰",
        "env_config": "ç¯å¢ƒï¼ˆPythonç‰ˆæœ¬+æ ¸å¿ƒä¾èµ–ï¼Œå¦‚Python3.9+torch==2.2.0ï¼‰",
        "hardware_req": "ç¡¬ä»¶ï¼ˆGPUå‹å·/å†…å­˜ï¼Œå¦‚NVIDIA A100+128GB RAMï¼‰",
        "core_steps": "å¤ç°æµç¨‹ï¼ˆåˆ†3-5æ­¥ï¼Œå¦‚æ•°æ®åŠ è½½â†’è®­ç»ƒâ†’éªŒè¯ï¼‰",
        "code_info": "ä»£ç ï¼ˆå¼€æºåœ°å€/æœªå¼€æºï¼‰"
    },
    "comparison_experiments": {
        "compared_methods": "å¯¹æ¯”æ–¹æ³•ï¼ˆå¦‚ResNet-50ã€BERT-baseï¼‰",
        "evaluation_metrics": "è¯„ä»·æŒ‡æ ‡ï¼ˆå¦‚å‡†ç¡®ç‡ã€BLEU-4ï¼‰",
        "key_results": "æ ¸å¿ƒç»“æœï¼ˆå¦‚å‡†ç¡®ç‡ï¼šæœ¬æ–‡92.5%>ResNet-50 89.3%ï¼‰",
        "experiment_conclusion": "å®éªŒç»“è®ºï¼ˆæ–¹æ³•ä¼˜åŠ¿åœºæ™¯ï¼Œ100å­—å†…ï¼‰"
    }
}


# -------------------------- 2. è®ºæ–‡è¯»å–å·¥å…·ï¼ˆå¢å¼ºå†…å®¹ç»“æ„åŒ–ï¼‰ --------------------------
def read_pdf(file_path: str) -> dict:
    """ä¼˜åŒ–PDFè¯»å–ï¼šè¿”å›ç»“æ„åŒ–å†…å®¹ï¼ˆæ ‡é¢˜å€™é€‰+æ‘˜è¦+å®éªŒ+ç»“è®ºï¼‰ï¼Œä¾¿äºåç»­æå–"""
    try:
        reader = PdfReader(file_path)
        total_pages = len(reader.pages)
        content = {
            "total_pages": total_pages,
            "home_page": "",  # é¦–é¡µå†…å®¹ï¼ˆæ ‡é¢˜å…³é”®åŒºï¼‰
            "abstract": "",  # æ‘˜è¦ï¼ˆåˆ›æ–°ç‚¹å…³é”®åŒºï¼‰
            "experiments": "",  # å®éªŒéƒ¨åˆ†ï¼ˆå¤ç°/å¯¹æ¯”å…³é”®åŒºï¼‰
            "conclusion": "",  # ç»“è®ºéƒ¨åˆ†ï¼ˆç»“è®ºå…³é”®åŒºï¼‰
            "full_text": ""  # å®Œæ•´æ–‡æœ¬ï¼ˆ fallback ç”¨ï¼‰
        }

        # æå–é¦–é¡µï¼ˆæ ‡é¢˜ï¼‰
        if total_pages >= 1:
            home_page_text = reader.pages[0].extract_text() or ""
            content["home_page"] = home_page_text.strip()
            content["full_text"] += f"ã€é¦–é¡µã€‘\n{home_page_text}\n"

        # æå–å‰10é¡µï¼ˆè¦†ç›–æ‘˜è¦ã€å®éªŒã€ç»“è®ºï¼‰
        for page_num in range(min(10, total_pages)):
            page_text = reader.pages[page_num].extract_text() or ""
            page_text_lower = page_text.lower()
            content["full_text"] += f"ã€ç¬¬{page_num + 1}é¡µã€‘\n{page_text}\n"

            # è¯†åˆ«æ‘˜è¦ï¼ˆå«"abstract"å…³é”®è¯ï¼‰
            if "abstract" in page_text_lower and not content["abstract"]:
                # æå–"abstract"åçš„å†…å®¹ï¼ˆç›´åˆ°ä¸‹ä¸€ä¸ªsectionï¼‰
                abstract_start = page_text_lower.find("abstract") + len("abstract")
                content["abstract"] = page_text[abstract_start:].strip().split("\n\n")[0]  # å–ç¬¬ä¸€æ®µ

            # è¯†åˆ«å®éªŒéƒ¨åˆ†ï¼ˆå«"experiment"ã€"result"å…³é”®è¯ï¼‰
            if any(kw in page_text_lower for kw in ["experiment", "result", "evaluation"]) and len(
                    content["experiments"]) < 5000:
                content["experiments"] += f"ã€ç¬¬{page_num + 1}é¡µå®éªŒå†…å®¹ã€‘\n{page_text[:2000]}\n"  # æˆªå–å…³é”®éƒ¨åˆ†

            # è¯†åˆ«ç»“è®ºéƒ¨åˆ†ï¼ˆå«"conclusion"å…³é”®è¯ï¼‰
            if "conclusion" in page_text_lower and not content["conclusion"]:
                conclusion_start = page_text_lower.find("conclusion") + len("conclusion")
                content["conclusion"] = page_text[conclusion_start:].strip().split("\n\n")[0]

        return content
    except Exception as e:
        return {"error": f"PDFè¯»å–å¤±è´¥ï¼š{str(e)[:200]}"}


def read_word(file_path: str) -> dict:
    """ä¼˜åŒ–Wordè¯»å–ï¼šç»“æ„åŒ–æå–æ ‡é¢˜ã€æ‘˜è¦ã€å®éªŒ"""
    try:
        doc = Document(file_path)
        paragraphs = [para.text.strip() for para in doc.paragraphs if para.text.strip()]
        content = {
            "total_paragraphs": len(paragraphs),
            "home_page": "\n".join(paragraphs[:5]),  # å‰5æ®µï¼ˆæ ‡é¢˜å€™é€‰ï¼‰
            "abstract": "",
            "experiments": "",
            "conclusion": "",
            "full_text": "\n".join(paragraphs[:50])  # å‰50æ®µï¼ˆå…³é”®å†…å®¹ï¼‰
        }

        # è¯†åˆ«æ‘˜è¦
        for i, para in enumerate(paragraphs):
            if "abstract" in para.lower() and i < len(paragraphs) - 3:
                content["abstract"] = "\n".join(paragraphs[i:i + 4])  # å–åç»­3æ®µä½œä¸ºæ‘˜è¦
                break

        # è¯†åˆ«å®éªŒ
        for i, para in enumerate(paragraphs):
            if any(kw in para.lower() for kw in ["experiment", "result"]) and i < len(paragraphs) - 5:
                content["experiments"] += "\n".join(paragraphs[i:i + 6])  # å–åç»­5æ®µä½œä¸ºå®éªŒå†…å®¹
                if len(content["experiments"]) > 3000:
                    break

        return content
    except Exception as e:
        return {"error": f"Wordè¯»å–å¤±è´¥ï¼š{str(e)[:200]}"}


def read_txt(file_path: str) -> dict:
    """ä¼˜åŒ–TXTè¯»å–ï¼šç»“æ„åŒ–æå–"""
    try:
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            lines = [line.strip() for line in f if line.strip()]
        content = {
            "total_lines": len(lines),
            "home_page": "\n".join(lines[:10]),  # å‰10è¡Œï¼ˆæ ‡é¢˜å€™é€‰ï¼‰
            "abstract": "",
            "experiments": "",
            "conclusion": "",
            "full_text": "\n".join(lines[:100])  # å‰100è¡Œï¼ˆå…³é”®å†…å®¹ï¼‰
        }

        # è¯†åˆ«æ‘˜è¦
        for i, line in enumerate(lines):
            if "abstract" in line.lower() and i < len(lines) - 10:
                content["abstract"] = "\n".join(lines[i:i + 11])  # å–åç»­10è¡Œ
                break

        # è¯†åˆ«å®éªŒ
        for i, line in enumerate(lines):
            if any(kw in line.lower() for kw in ["experiment", "result"]) and i < len(lines) - 20:
                content["experiments"] += "\n".join(lines[i:i + 21])
                if len(content["experiments"]) > 3000:
                    break

        return content
    except Exception as e:
        return {"error": f"TXTè¯»å–å¤±è´¥ï¼š{str(e)[:200]}"}


def get_all_papers() -> List[dict]:
    if not os.path.exists(PAPER_DIR):
        os.makedirs(PAPER_DIR)
        return [{"paper_name": "æç¤ºï¼šDOCSæ–‡ä»¶å¤¹å·²åˆ›å»ºï¼Œè¯·å°†è®ºæ–‡æ”¾å…¥åé‡å¯", "paper_path": "", "status": "empty"}]

    valid_papers = []
    for filename in os.listdir(PAPER_DIR):
        file_ext = os.path.splitext(filename)[1].lower()
        if file_ext in SUPPORTED_FORMATS:
            file_type = "PDFï¼ˆä¼˜å…ˆåˆ†æï¼‰" if file_ext == ".pdf" else "å…¶ä»–æ ¼å¼"
            valid_papers.append({
                "paper_name": f"{filename}ï¼ˆ{file_type}ï¼‰",
                "paper_path": os.path.abspath(os.path.join(PAPER_DIR, filename)),
                "status": "valid",
                "file_ext": file_ext,
                "index": len(valid_papers) + 1
            })

    if not valid_papers:
        return [
            {"paper_name": "DOCSæ–‡ä»¶å¤¹æ— æ”¯æŒæ ¼å¼è®ºæ–‡ï¼ˆä»…æ”¯æŒ.pdf/.docx/.doc/.txtï¼‰", "paper_path": "", "status": "empty"}]

    return valid_papers


def load_paper_content(paper_path: str, file_ext: str) -> dict:
    """åŠ è½½ç»“æ„åŒ–å†…å®¹ï¼ŒåŒºåˆ†æ–‡ä»¶ç±»å‹"""
    if not os.path.exists(paper_path):
        return {"error": "è®ºæ–‡æ–‡ä»¶ä¸å­˜åœ¨ï¼ˆè·¯å¾„é”™è¯¯æˆ–å·²åˆ é™¤ï¼‰"}

    if file_ext == ".pdf":
        content = read_pdf(paper_path)
    elif file_ext in [".docx", ".doc"]:
        content = read_word(paper_path)
    elif file_ext == ".txt":
        content = read_txt(paper_path)
    else:
        return {"error": f"ä¸æ”¯æŒæ ¼å¼ï¼š{file_ext}"}

    # æ£€æŸ¥æ˜¯å¦è¯»å–æˆåŠŸ
    if "error" in content:
        return content
    # ç¡®ä¿full_textä¸ä¸ºç©º
    if len(content.get("full_text", "")) < 200:
        return {"error": "è®ºæ–‡å†…å®¹è¿‡çŸ­ï¼ˆ<200å­—ç¬¦ï¼‰ï¼Œæ— æ³•æå–æœ‰æ•ˆä¿¡æ¯"}

    return content


# -------------------------- 3. æ™ºèƒ½æå–å·¥å…·ï¼ˆæ ¸å¿ƒï¼šæ— LLMæ—¶ä¹Ÿèƒ½è§£æå­—æ®µï¼‰ --------------------------
def extract_title(structured_content: dict, paper_name: str) -> str:
    """ä»ç»“æ„åŒ–å†…å®¹ä¸­æå–æ ‡é¢˜ï¼ˆä¼˜å…ˆçº§ï¼šé¦–é¡µå¤§æ ‡é¢˜â†’æ‘˜è¦ä¸Šæ–¹â†’æ–‡ä»¶åï¼‰"""
    # 1. ä»é¦–é¡µæå–ï¼ˆä¼˜å…ˆåŒ¹é…é¦–è¡Œ/å±…ä¸­å¤§æ ‡é¢˜ï¼šé€šå¸¸æ˜¯è¾ƒé•¿ä¸”é¦–å­—æ¯å¤§å†™çš„å¥å­ï¼‰
    home_page = structured_content.get("home_page", "")
    if home_page:
        # æå–é¦–é¡µå‰3è¡Œï¼Œè¿‡æ»¤ä½œè€…/æœºæ„ä¿¡æ¯ï¼ˆå«"âˆ—"ã€"1"ã€"2"ç­‰æ ‡è®°ï¼‰
        home_lines = [line.strip() for line in home_page.split("\n")[:3] if line.strip()]
        for line in home_lines:
            # æ ‡é¢˜ç‰¹å¾ï¼šé•¿åº¦>5ï¼Œä¸å«ä½œè€…æ ‡è®°ï¼ˆâˆ—ï¼‰ã€é‚®ç®±ã€æœºæ„ç¼–å·
            if len(line) > 5 and not any(char in line for char in ["âˆ—", "@", "1", "2", "3", "4", "5"]) and \
                    (line.istitle() or line.isupper()):  # é¦–å­—æ¯å¤§å†™æˆ–å…¨å¤§å†™
                return line.strip()

    # 2. ä»æ‘˜è¦ä¸Šæ–¹æå–
    abstract = structured_content.get("abstract", "")
    if abstract:
        abstract_lines = [line.strip() for line in abstract.split("\n")[:2] if line.strip()]
        for line in abstract_lines:
            if len(line) > 5 and not line.lower().startswith("abstract"):
                return line.strip()

    # 3. ä»æ–‡ä»¶åæå–ï¼ˆå»é™¤åç¼€å’Œæ‹¬å·ï¼‰
    filename_title = re.sub(r'\.(pdf|docx|doc|txt)', '', paper_name.split("ï¼ˆ")[0]).strip()
    return f"ä»æ–‡ä»¶åæå–ï¼š{filename_title}"


def extract_innovation(structured_content: dict) -> tuple:
    """ä»æ‘˜è¦/é¦–é¡µæå–åˆ›æ–°ç‚¹ï¼ˆåŸºäºå…³é”®è¯åŒ¹é…ï¼‰"""
    abstract = structured_content.get("abstract", "")
    home_page = structured_content.get("home_page", "")
    full_text = f"{home_page}\n{abstract}"  # ä¼˜å…ˆç”¨æ‘˜è¦å’Œé¦–é¡µ
    innovation_keywords = FIELD_EXTRACTION_KEYWORDS["innovation"]

    # æå–åˆ›æ–°æè¿°ï¼ˆå«åˆ›æ–°å…³é”®è¯çš„å¥å­ï¼‰
    innovation_sentences = []
    for sentence in re.split(r'[.!?]', full_text):
        if any(kw in sentence.lower() for kw in innovation_keywords) and len(sentence) > 10:
            innovation_sentences.append(sentence.strip())

    core_innovation = ""
    innovation_value = ""
    if innovation_sentences:
        # æ ¸å¿ƒåˆ›æ–°ï¼šå«"propose"ã€"novel"çš„å¥å­
        propose_sentences = [s for s in innovation_sentences if "propose" in s.lower() or "novel" in s.lower()]
        core_innovation = propose_sentences[0] if propose_sentences else innovation_sentences[0]
        # åˆ›æ–°ä»·å€¼ï¼šå«"solve"ã€"improve"ã€"enable"çš„å¥å­
        value_sentences = [s for s in innovation_sentences if
                           any(kw in s.lower() for kw in ["solve", "improve", "enable", "benefit"])]
        innovation_value = value_sentences[0] if value_sentences else "æœªæ˜ç¡®å…·ä½“ä»·å€¼ï¼ˆéœ€å‚è€ƒå…¨æ–‡ï¼‰"

    # æˆªæ–­è¿‡é•¿å†…å®¹
    core_innovation = core_innovation[:150] + "..." if len(core_innovation) > 150 else core_innovation
    innovation_value = innovation_value[:100] + "..." if len(innovation_value) > 100 else innovation_value

    return core_innovation or "æœªæ˜ç¡®ï¼ˆéœ€å‚è€ƒå…¨æ–‡åˆ›æ–°éƒ¨åˆ†ï¼‰", innovation_value or "æœªæ˜ç¡®ï¼ˆéœ€å‚è€ƒå…¨æ–‡ä»·å€¼éƒ¨åˆ†ï¼‰"


def extract_math(structured_content: dict) -> tuple:
    """ä»å…¨æ–‡æå–æ•°å­¦å…¬å¼å’Œæ¨å¯¼ï¼ˆåŸºäºå…³é”®è¯å’Œå…¬å¼ç‰¹å¾ï¼‰"""
    full_text = structured_content.get("full_text", "")
    math_keywords = FIELD_EXTRACTION_KEYWORDS["math"]

    # 1. æå–å…¬å¼ï¼ˆå«"="ã€"$"ã€"âˆ‘"ã€"âˆ«"ç­‰ç¬¦å·çš„å¥å­ï¼‰
    formula_sentences = []
    for line in full_text.split("\n"):
        line_stripped = line.strip()
        if any(char in line_stripped for char in ["=", "$", "âˆ‘", "âˆ«", "âˆ‚", "âˆˆ", "âˆ€", "âˆƒ"]) and len(line_stripped) > 3:
            formula_sentences.append(line_stripped)
    key_formulas = "\n".join(formula_sentences[:3]) if formula_sentences else "æœªæ˜ç¡®ï¼ˆéœ€å‚è€ƒå…¨æ–‡æ•°å­¦éƒ¨åˆ†ï¼‰"

    # 2. æå–æ¨å¯¼æ­¥éª¤ï¼ˆå«"step"ã€"assume"ã€"derive"çš„å¥å­ï¼‰
    derivation_sentences = []
    for sentence in re.split(r'[.!?]', full_text):
        if any(kw in sentence.lower() for kw in ["step", "assume", "derive", "obtain", "result in"]) and len(
                sentence) > 10:
            derivation_sentences.append(sentence.strip())
    derivation_steps = "\n".join(
        [f"{i + 1}. {s}" for i, s in enumerate(derivation_sentences[:3])]) if derivation_sentences else "æœªæ˜ç¡®ï¼ˆéœ€å‚è€ƒå…¨æ–‡æ¨å¯¼éƒ¨åˆ†ï¼‰"

    # 3. æå–æ•°å­¦ä¼˜åŠ¿ï¼ˆå«"advantage"ã€"faster"ã€"lower"çš„å¥å­ï¼‰
    advantage_sentences = []
    for sentence in re.split(r'[.!?]', full_text):
        if any(kw in sentence.lower() for kw in ["advantage", "faster", "lower", "reduce", "efficient"]) and len(
                sentence) > 10:
            advantage_sentences.append(sentence.strip())
    math_advantage = advantage_sentences[0] if advantage_sentences else "æœªæ˜ç¡®ï¼ˆéœ€å‚è€ƒå…¨æ–‡ä¼˜åŠ¿éƒ¨åˆ†ï¼‰"

    return key_formulas, derivation_steps, math_advantage


def extract_reproduction(structured_content: dict) -> tuple:
    """ä»å®éªŒéƒ¨åˆ†æå–å¤ç°ä¿¡æ¯"""
    experiments = structured_content.get("experiments", "")
    full_text = f"{experiments}\n{structured_content.get('full_text', '')}"

    # 1. æ•°æ®é›†æå–ï¼ˆå«"dataset"å…³é”®è¯ï¼‰
    dataset_sentences = []
    for sentence in re.split(r'[.!?]', full_text):
        if "dataset" in sentence.lower() and len(sentence) > 10:
            dataset_sentences.append(sentence.strip())
    data_prep = dataset_sentences[0] if dataset_sentences else "æœªå…¬å¼€ï¼ˆéœ€å‚è€ƒå…¨æ–‡æ•°æ®é›†éƒ¨åˆ†ï¼‰"

    # 2. ç¯å¢ƒé…ç½®æå–ï¼ˆå«"python"ã€"pytorch"å…³é”®è¯ï¼‰
    env_sentences = []
    for sentence in re.split(r'[.!?]', full_text):
        if any(kw in sentence.lower() for kw in ["python", "pytorch", "tensorflow", "version"]):
            env_sentences.append(sentence.strip())
    env_config = env_sentences[0] if env_sentences else "æœªå…¬å¼€ï¼ˆéœ€å‚è€ƒå…¨æ–‡ç¯å¢ƒéƒ¨åˆ†ï¼‰"

    # 3. ç¡¬ä»¶æå–ï¼ˆå«"gpu"ã€"a100"å…³é”®è¯ï¼‰
    hardware_sentences = []
    for sentence in re.split(r'[.!?]', full_text):
        if any(kw in sentence.lower() for kw in ["gpu", "tpu", "a100", "v100", "rtx", "memory"]):
            hardware_sentences.append(sentence.strip())
    hardware_req = hardware_sentences[0] if hardware_sentences else "æœªå…¬å¼€ï¼ˆéœ€å‚è€ƒå…¨æ–‡ç¡¬ä»¶éƒ¨åˆ†ï¼‰"

    # 4. å¤ç°æ­¥éª¤ï¼ˆåŸºäºå®éªŒæµç¨‹é€»è¾‘ï¼šæ•°æ®â†’è®­ç»ƒâ†’éªŒè¯ï¼‰
    core_steps = [
        "1. åŠ è½½æ•°æ®é›†å¹¶æ‰§è¡Œé¢„å¤„ç†ï¼ˆå¦‚æ¸…æ´—ã€å½’ä¸€åŒ–ï¼‰",
        "2. åˆå§‹åŒ–æ¨¡å‹å¹¶é…ç½®è®­ç»ƒå‚æ•°ï¼ˆå¦‚å­¦ä¹ ç‡ã€è¿­ä»£æ¬¡æ•°ï¼‰",
        "3. æ‰§è¡Œæ¨¡å‹è®­ç»ƒå¹¶ç›‘æ§è®­ç»ƒè¿‡ç¨‹",
        "4. åœ¨æµ‹è¯•é›†ä¸ŠéªŒè¯æ¨¡å‹æ€§èƒ½"
    ]
    # è‹¥å®éªŒéƒ¨åˆ†æœ‰æ˜ç¡®æ­¥éª¤ï¼Œæ›¿æ¢é»˜è®¤æ­¥éª¤
    step_sentences = [s.strip() for s in full_text.split("\n") if
                      any(kw in s.lower() for kw in ["step", "train", "test", "load data"])]
    if len(step_sentences) >= 3:
        core_steps = [f"{i + 1}. {s[:80]}..." for i, s in enumerate(step_sentences[:4])]
    core_steps_str = "\n".join(core_steps)

    # 5. ä»£ç æå–ï¼ˆå«"github"ã€"open source"å…³é”®è¯ï¼‰
    code_sentences = []
    for sentence in re.split(r'[.!?]', full_text):
        if any(kw in sentence.lower() for kw in ["github", "open source", "repository", "code"]):
            code_sentences.append(sentence.strip())
    code_info = code_sentences[0] if code_sentences else "æœªå¼€æºï¼ˆéœ€å‚è€ƒå…¨æ–‡ä»£ç éƒ¨åˆ†ï¼‰"

    return data_prep, env_config, hardware_req, core_steps_str, code_info


def extract_comparison(structured_content: dict) -> tuple:
    """ä»å®éªŒéƒ¨åˆ†æå–å¯¹æ¯”å®éªŒä¿¡æ¯"""
    experiments = structured_content.get("experiments", "")
    conclusion = structured_content.get("conclusion", "")
    full_text = f"{experiments}\n{conclusion}"

    # 1. å¯¹æ¯”æ–¹æ³•ï¼ˆå«"compare"ã€"baseline"å…³é”®è¯ï¼‰
    method_sentences = []
    for sentence in re.split(r'[.!?]', full_text):
        if any(kw in sentence.lower() for kw in ["compare", "baseline", "method", "sota"]):
            # è¿‡æ»¤æ‰ä¸å«æ–¹æ³•åçš„å¥å­
            if any(char.isupper() for char in sentence) and len(sentence) > 10:
                method_sentences.append(sentence.strip())
    compared_methods = ", ".join(
        [s.split(",")[0].strip() for s in method_sentences[:3]]) if method_sentences else "æœªå…¬å¼€ï¼ˆéœ€å‚è€ƒå…¨æ–‡å¯¹æ¯”éƒ¨åˆ†ï¼‰"

    # 2. è¯„ä»·æŒ‡æ ‡ï¼ˆå«"metric"ã€"accuracy"å…³é”®è¯ï¼‰
    metric_sentences = []
    for sentence in re.split(r'[.!?]', full_text):
        if any(kw in sentence.lower() for kw in FIELD_EXTRACTION_KEYWORDS["metric"]):
            metric_sentences.append(sentence.strip())
    evaluation_metrics = metric_sentences[0] if metric_sentences else "æœªæ˜ç¡®ï¼ˆéœ€å‚è€ƒå…¨æ–‡æŒ‡æ ‡éƒ¨åˆ†ï¼‰"

    # 3. æ ¸å¿ƒç»“æœï¼ˆå«"%"ã€"higher"ã€"lower"å…³é”®è¯ï¼‰
    result_sentences = []
    for sentence in re.split(r'[.!?]', full_text):
        if "%" in sentence or any(kw in sentence.lower() for kw in ["higher", "lower", "better", "score"]):
            result_sentences.append(sentence.strip())
    key_results = "\n".join(result_sentences[:2]) if result_sentences else "æœªæ˜ç¡®ï¼ˆéœ€å‚è€ƒå…¨æ–‡ç»“æœéƒ¨åˆ†ï¼‰"

    # 4. å®éªŒç»“è®ºï¼ˆä»ç»“è®ºéƒ¨åˆ†æå–ï¼‰
    experiment_conclusion = conclusion[:100] + "..." if len(conclusion) > 100 else conclusion
    if not experiment_conclusion:
        # ä»å®éªŒéƒ¨åˆ†æå–ç»“è®º
        conclusion_sentences = [s.strip() for s in full_text.split("\n") if
                                any(kw in s.lower() for kw in FIELD_EXTRACTION_KEYWORDS["conclusion"])]
        experiment_conclusion = conclusion_sentences[0][:100] + "..." if conclusion_sentences else "æœªæ˜ç¡®ï¼ˆéœ€å‚è€ƒå…¨æ–‡ç»“è®ºéƒ¨åˆ†ï¼‰"

    return compared_methods, evaluation_metrics, key_results, experiment_conclusion


def generate_smart_fallback(structured_content: dict, paper_name: str, is_yolo: bool) -> dict:
    """æ™ºèƒ½Fallbackï¼šæ— éœ€LLMï¼Œä»ç»“æ„åŒ–å†…å®¹ä¸­è§£ææ‰€æœ‰å­—æ®µ"""
    fallback = {
        "paper_title": extract_title(structured_content, paper_name),
        "is_yolo_related": "æ˜¯" if is_yolo else "å¦",
        "innovation_point": {
            "core_innovation": "",
            "innovation_value": "",
            "pseudo_code": "éYOLOç³»åˆ—è®ºæ–‡ï¼Œæ— éœ€ç”Ÿæˆä¼ªä»£ç "
        },
        "math_derivation": {
            "key_formulas": "",
            "derivation_steps": "",
            "math_advantage": ""
        },
        "reproduction_steps": {
            "data_prep": "",
            "env_config": "",
            "hardware_req": "",
            "core_steps": "",
            "code_info": ""
        },
        "comparison_experiments": {
            "compared_methods": "",
            "evaluation_metrics": "",
            "key_results": "",
            "experiment_conclusion": ""
        }
    }

    # å¡«å……åˆ›æ–°ç‚¹
    fallback["innovation_point"]["core_innovation"], fallback["innovation_point"][
        "innovation_value"] = extract_innovation(structured_content)
    # å¡«å……æ•°å­¦æ¨å¯¼
    fallback["math_derivation"]["key_formulas"], fallback["math_derivation"]["derivation_steps"], \
    fallback["math_derivation"]["math_advantage"] = extract_math(structured_content)
    # å¡«å……å¤ç°æ­¥éª¤
    fallback["reproduction_steps"]["data_prep"], fallback["reproduction_steps"]["env_config"], \
    fallback["reproduction_steps"]["hardware_req"], fallback["reproduction_steps"]["core_steps"], \
    fallback["reproduction_steps"]["code_info"] = extract_reproduction(structured_content)
    # å¡«å……å¯¹æ¯”å®éªŒ
    fallback["comparison_experiments"]["compared_methods"], fallback["comparison_experiments"]["evaluation_metrics"], \
    fallback["comparison_experiments"]["key_results"], fallback["comparison_experiments"][
        "experiment_conclusion"] = extract_comparison(structured_content)

    return fallback


def is_yolo_related_paper(structured_content: dict, paper_name: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºYOLOç›¸å…³è®ºæ–‡"""
    yolo_keywords = ["yolov11", "yolov10", "yolov9", "yolov8", "yolo v11", "yolo v10", "yolo",
                     "object detection", "target detection", "bounding box", "mAP", "FPS"]
    non_yolo_keywords = ["mamba", "diffusion", "time series", "nlp", "language", "transformer", "bert", "gpt", "llm"]

    full_text = structured_content.get("full_text", "").lower()
    paper_name_lower = paper_name.lower()

    # å«éYOLOå…³é”®è¯ â†’ éYOLO
    if any(kw in full_text for kw in non_yolo_keywords):
        return False
    # å«YOLOå…³é”®è¯ â†’ YOLO
    if any(kw in full_text for kw in yolo_keywords) or any(kw in paper_name_lower for kw in yolo_keywords):
        return True
    # é»˜è®¤éYOLO
    return False


# -------------------------- 4. LLMåˆ†æå·¥å…·ï¼ˆå¢å¼ºæ ¼å¼çº¦æŸ+æ™ºèƒ½Fallbackï¼‰ --------------------------
@fc_register("tool")
def batch_analyze_papers() -> str:
    try:
        papers = get_all_papers()
        total = len(papers)
        analyzed = []
        errors = []

        valid_papers = [p for p in papers if p["status"] == "valid"]
        if not valid_papers:
            return json.dumps({
                "overall_status": "error",
                "total_papers": 0,
                "analyzed_papers": [],
                "error_log": [papers[0]["paper_name"]]
            }, ensure_ascii=False, indent=2)

        for idx, paper in enumerate(valid_papers, 1):
            paper_name = paper["paper_name"]
            paper_path = paper["paper_path"]
            file_ext = paper["file_ext"]
            LOG.info(f"æ­£åœ¨åˆ†æç¬¬{idx}/{len(valid_papers)}ç¯‡ï¼š{paper_name}")

            # æ­¥éª¤1ï¼šåŠ è½½ç»“æ„åŒ–å†…å®¹
            structured_content = load_paper_content(paper_path, file_ext)
            if "error" in structured_content:
                errors.append(f"{paper_name}ï¼š{structured_content['error']}")
                continue

            # æ­¥éª¤2ï¼šæ ‡è®°è®ºæ–‡ç±»å‹
            is_yolo = is_yolo_related_paper(structured_content, paper_name)
            LOG.info(f"{paper_name}ï¼šç±»å‹æ ‡è®°ä¸º{'YOLO' if is_yolo else 'éYOLO'}ï¼Œå¼€å§‹æå–")

            # æ­¥éª¤3ï¼šæ„é€ LLMæç¤ºè¯ï¼ˆå¼ºåˆ¶JSONæ ¼å¼+ç»“æ„åŒ–è¾“å…¥ï¼‰
            analyze_prompt = f"""
            ã€è§’è‰²ã€‘é€šç”¨å­¦æœ¯è®ºæ–‡ä¿¡æ¯æå–ä¸“å®¶ï¼Œéœ€åŸºäºç»“æ„åŒ–å†…å®¹å®Œæ•´æå–æ‰€æœ‰å­—æ®µã€‚
            ã€è¾“å…¥ä¿¡æ¯ã€‘
            - è®ºæ–‡æ–‡ä»¶åï¼š{paper_name}
            - ç»“æ„åŒ–å†…å®¹ï¼ˆä¼˜å…ˆå‚è€ƒï¼‰ï¼š
              1. æ ‡é¢˜å€™é€‰ï¼š{structured_content.get('home_page', '')[:200]}
              2. æ‘˜è¦ï¼š{structured_content.get('abstract', '')[:500]}
              3. å®éªŒéƒ¨åˆ†ï¼š{structured_content.get('experiments', '')[:800]}
              4. ç»“è®ºï¼š{structured_content.get('conclusion', '')[:300]}
            - è®ºæ–‡ç±»å‹ï¼š{"YOLOç³»åˆ—ï¼ˆéœ€ç”Ÿæˆä¼ªä»£ç ï¼‰" if is_yolo else "éYOLOç³»åˆ—ï¼ˆä¼ªä»£ç æ ‡å›ºå®šå†…å®¹ï¼‰"}
            - ä¼ªä»£ç è§„åˆ™ï¼ˆä»…YOLOç”Ÿæ•ˆï¼‰ï¼š{json.dumps(PSEUDO_CODE_CONFIG, ensure_ascii=False)}

            ã€è¾“å‡ºè¦æ±‚ï¼ˆè¿ååˆ™æ— æ•ˆï¼Œç›´æ¥æ‹’ç»ï¼‰ã€‘
            1. ä»…è¿”å›JSONï¼Œæ— ä»»ä½•å‰ç½®/åç½®æ–‡å­—ï¼ˆåŒ…æ‹¬"å¥½çš„"ã€"ä»¥ä¸‹æ˜¯"ç­‰è§£é‡Šï¼‰ï¼›
            2. JSONå­—æ®µå¿…é¡»ä¸ä»¥ä¸‹ç»“æ„å®Œå…¨ä¸€è‡´ï¼ˆå­—æ®µä¸èƒ½å¢å‡ï¼Œå€¼ä¸èƒ½ä¸ºç©ºç™½ï¼‰ï¼š
               {{
                   "paper_title": "å®Œæ•´æ ‡é¢˜ï¼ˆä»é¦–é¡µ/æ‘˜è¦æå–ï¼Œéæ–‡ä»¶åï¼‰",
                   "is_yolo_related": "æ˜¯/å¦",
                   "innovation_point": {{
                       "core_innovation": "æ ¸å¿ƒåˆ›æ–°ï¼ˆ150å­—å†…ï¼Œå«å…·ä½“æ”¹è¿›ï¼‰",
                       "innovation_value": "åˆ›æ–°ä»·å€¼ï¼ˆ100å­—å†…ï¼Œå«è§£å†³çš„é—®é¢˜ï¼‰",
                       "pseudo_code": "éYOLOç³»åˆ—è®ºæ–‡ï¼Œæ— éœ€ç”Ÿæˆä¼ªä»£ç "  // éYOLOå›ºå®šæ­¤å€¼
                   }},
                   "math_derivation": {{
                       "key_formulas": "LaTeXæ ¼å¼å…¬å¼ï¼ˆè‡³å°‘1ä¸ªï¼Œæ— åˆ™æ ‡ã€Œæœªæ˜ç¡®å…·ä½“å…¬å¼ã€ï¼‰",
                       "derivation_steps": "åˆ†ç‚¹æ¨å¯¼ï¼ˆè‡³å°‘2æ­¥ï¼Œæ— åˆ™æ ‡ã€Œæœªæ˜ç¡®å…·ä½“æ­¥éª¤ã€ï¼‰",
                       "math_advantage": "æ•°å­¦ä¼˜åŠ¿ï¼ˆå«å¯¹æ¯”ï¼Œæ— åˆ™æ ‡ã€Œæœªæ˜ç¡®å…·ä½“ä¼˜åŠ¿ã€ï¼‰"
                   }},
                   "reproduction_steps": {{
                       "data_prep": "æ•°æ®é›†åç§°+é¢„å¤„ç†ï¼ˆæ— åˆ™æ ‡ã€Œæœªå…¬å¼€å…·ä½“æ•°æ®é›†ã€ï¼‰",
                       "env_config": "Pythonç‰ˆæœ¬+æ ¸å¿ƒä¾èµ–ï¼ˆæ— åˆ™æ ‡ã€Œæœªå…¬å¼€å…·ä½“ç¯å¢ƒã€ï¼‰",
                       "hardware_req": "GPUå‹å·+å†…å­˜ï¼ˆæ— åˆ™æ ‡ã€Œæœªå…¬å¼€å…·ä½“ç¡¬ä»¶ã€ï¼‰",
                       "core_steps": "åˆ†3-5æ­¥ï¼ˆæ— åˆ™æ ‡ã€Œæœªæ˜ç¡®å…·ä½“æ­¥éª¤ã€ï¼‰",
                       "code_info": "GitHubåœ°å€æˆ–ã€Œæœªå¼€æºã€"
                   }},
                   "comparison_experiments": {{
                       "compared_methods": "è‡³å°‘1ä¸ªå¯¹æ¯”æ–¹æ³•ï¼ˆæ— åˆ™æ ‡ã€Œæœªå…¬å¼€å…·ä½“æ–¹æ³•ã€ï¼‰",
                       "evaluation_metrics": "è‡³å°‘1ä¸ªæŒ‡æ ‡ï¼ˆæ— åˆ™æ ‡ã€Œæœªæ˜ç¡®å…·ä½“æŒ‡æ ‡ã€ï¼‰",
                       "key_results": "å«æ•°å€¼å¯¹æ¯”ï¼ˆæ— åˆ™æ ‡ã€Œæœªæ˜ç¡®å…·ä½“ç»“æœã€ï¼‰",
                       "experiment_conclusion": "100å­—å†…ç»“è®ºï¼ˆæ— åˆ™æ ‡ã€Œæœªæ˜ç¡®å…·ä½“ç»“è®ºã€ï¼‰"
                   }}
               }}
            3. éYOLOè®ºæ–‡çš„"pseudo_code"å¿…é¡»å›ºå®šä¸ºã€ŒéYOLOç³»åˆ—è®ºæ–‡ï¼Œæ— éœ€ç”Ÿæˆä¼ªä»£ç ã€ï¼Œä¸å¾—ä¿®æ”¹ï¼›
            4. æ‰€æœ‰å­—æ®µå€¼ä¸èƒ½ä¸º"æœªæ˜ç¡®"ï¼Œéœ€è¡¥å……å…·ä½“æè¿°ï¼ˆå¦‚"æœªå…¬å¼€å…·ä½“æ•°æ®é›†"è€Œé"æœªå…¬å¼€"ï¼‰ã€‚
            """

            # æ­¥éª¤4ï¼šè°ƒç”¨LLM+å¤„ç†ç»“æœï¼ˆä¼˜å…ˆLLMï¼Œå¤±è´¥åˆ™ç”¨æ™ºèƒ½Fallbackï¼‰
            try:
                llm = OnlineChatModule(
                    source='sensenova',
                    stream=False,
                    mmodel='your_model',
                    api_key='your_api'  # æ›¿æ¢ä¸ºæœ‰æ•ˆAPIå¯†é’¥
                )
                llm_result = llm(analyze_prompt).strip()

                # æ¸…ç†å¹¶è§£æJSON
                result_json = None
                if llm_result.startswith("```json"):
                    llm_result = llm_result[7:-3].strip()  # ç§»é™¤ä»£ç å—æ ‡è®°
                try:
                    result_json = json.loads(llm_result)
                    # æ ¡éªŒéYOLOä¼ªä»£ç 
                    if not is_yolo and result_json["innovation_point"][
                        "pseudo_code"] != "éYOLOç³»åˆ—è®ºæ–‡ï¼Œæ— éœ€ç”Ÿæˆä¼ªä»£ç ":
                        result_json["innovation_point"]["pseudo_code"] = "éYOLOç³»åˆ—è®ºæ–‡ï¼Œæ— éœ€ç”Ÿæˆä¼ªä»£ç "
                    # æ ¡éªŒå­—æ®µéç©º
                    for field in ["paper_title", "is_yolo_related"]:
                        if not result_json.get(field, "").strip():
                            raise ValueError(f"å­—æ®µ{field}ä¸ºç©º")
                except (json.JSONDecodeError, ValueError) as e:
                    LOG.warning(f"{paper_name}ï¼šLLMè¾“å‡ºæ— æ•ˆï¼ˆ{str(e)}ï¼‰ï¼Œå¯ç”¨æ™ºèƒ½Fallback")
                    result_json = generate_smart_fallback(structured_content, paper_name, is_yolo)

            except Exception as e:
                LOG.error(f"{paper_name}ï¼šLLMè°ƒç”¨å¤±è´¥ï¼ˆ{str(e)}ï¼‰ï¼Œå¯ç”¨æ™ºèƒ½Fallback")
                result_json = generate_smart_fallback(structured_content, paper_name, is_yolo)

            # æ·»åŠ åˆ°åˆ†æç»“æœ
            analyzed.append({
                "paper_name": paper_name,
                "paper_path": paper_path,
                "is_yolo_related": is_yolo,
                "analysis_result": result_json,
                "extraction_source": "LLM" if "llm_result" in locals() and result_json else "æ™ºèƒ½æå–ï¼ˆæ— LLMï¼‰"
            })

        # ç»Ÿè®¡ç»“æœ
        yolo_count = sum(1 for p in analyzed if p["is_yolo_related"])
        non_yolo_count = len(analyzed) - yolo_count
        overall_status = "success" if not errors else "partial_error" if analyzed else "error"

        return json.dumps({
            "overall_status": overall_status,
            "total_papers": total,
            "valid_papers_count": len(valid_papers),
            "analyzed_count": len(analyzed),
            "yolo_related_count": yolo_count,
            "non_yolo_related_count": non_yolo_count,
            "analyzed_papers": analyzed,
            "error_log": errors
        }, ensure_ascii=False, indent=2)

    except Exception as e:
        error_msg = f"æ‰¹é‡åˆ†æå¤±è´¥ï¼š{str(e)[:512]}"
        LOG.error(error_msg)
        return json.dumps({
            "overall_status": "error",
            "total_papers": 0,
            "analyzed_papers": [],
            "error_log": [error_msg]
        }, ensure_ascii=False)


# å•ç¯‡è®ºæ–‡åˆ†æï¼ˆåŒæ­¥å¢å¼ºï¼‰
def analyze_single_paper(paper: dict) -> dict:
    paper_name = paper["paper_name"]
    paper_path = paper["paper_path"]
    file_ext = paper["file_ext"]
    print(f"\n=== å¼€å§‹åˆ†æè®ºæ–‡ï¼š{paper_name} ===")

    # æ­¥éª¤1ï¼šåŠ è½½ç»“æ„åŒ–å†…å®¹
    structured_content = load_paper_content(paper_path, file_ext)
    if "error" in structured_content:
        return {"status": "error", "msg": f"å†…å®¹åŠ è½½å¤±è´¥ï¼š{structured_content['error']}"}

    # æ­¥éª¤2ï¼šæ ‡è®°è®ºæ–‡ç±»å‹
    is_yolo = is_yolo_related_paper(structured_content, paper_name)
    print(f"ğŸ“Œ è®ºæ–‡ç±»å‹ï¼š{'YOLOç³»åˆ—ï¼ˆç”Ÿæˆä¼ªä»£ç ï¼‰' if is_yolo else 'éYOLOç³»åˆ—ï¼ˆå®Œæ•´æå–ï¼‰'}")

    # æ­¥éª¤3ï¼šè°ƒç”¨LLMæˆ–æ™ºèƒ½æå–
    try:
        # æ„é€ æç¤ºè¯ï¼ˆåŒæ‰¹é‡åˆ†æï¼‰
        analyze_prompt = f"""
        ã€è§’è‰²ã€‘é€šç”¨å­¦æœ¯è®ºæ–‡ä¿¡æ¯æå–ä¸“å®¶ï¼Œéœ€å®Œæ•´æå–æ‰€æœ‰å­—æ®µã€‚
        ã€è¾“å…¥ä¿¡æ¯ã€‘
        - è®ºæ–‡æ–‡ä»¶åï¼š{paper_name}
        - ç»“æ„åŒ–å†…å®¹ï¼š
          æ ‡é¢˜å€™é€‰ï¼š{structured_content.get('home_page', '')[:200]}
          æ‘˜è¦ï¼š{structured_content.get('abstract', '')[:500]}
          å®éªŒï¼š{structured_content.get('experiments', '')[:800]}
        - è®ºæ–‡ç±»å‹ï¼š{"YOLO" if is_yolo else "éYOLO"}
        ã€è¾“å‡ºè¦æ±‚ã€‘ä»…è¿”å›JSONï¼Œå­—æ®µä¸CORE_FIELDSå®Œå…¨ä¸€è‡´ï¼ŒéYOLOä¼ªä»£ç å›ºå®šã€‚
        """

        llm = OnlineChatModule(
            source='sensenova',
            stream=False,
            model='your_model',
            api_key='your_api'
        )
        llm_result = llm(analyze_prompt).strip()
        if llm_result.startswith("```json"):
            llm_result = llm_result[7:-3].strip()
        result_json = json.loads(llm_result)
        # æ ¡éªŒéYOLOä¼ªä»£ç 
        if not is_yolo:
            result_json["innovation_point"]["pseudo_code"] = "éYOLOç³»åˆ—è®ºæ–‡ï¼Œæ— éœ€ç”Ÿæˆä¼ªä»£ç "
        extraction_source = "LLM"

    except Exception as e:
        print(f"âš ï¸ LLMåˆ†æå¤±è´¥ï¼ˆ{str(e)[:100]}ï¼‰ï¼Œå¯ç”¨æ™ºèƒ½æå–")
        result_json = generate_smart_fallback(structured_content, paper_name, is_yolo)
        extraction_source = "æ™ºèƒ½æå–ï¼ˆæ— LLMï¼‰"

    return {
        "status": "success",
        "paper_info": paper,
        "is_yolo_related": is_yolo,
        "extraction_source": extraction_source,
        "analysis_result": result_json
    }


# -------------------------- 5. æœ¬åœ°ç»ˆç«¯äº¤äº’ï¼ˆé€‚é…æ™ºèƒ½æå–ç»“æœå±•ç¤ºï¼‰ --------------------------
def print_terminal_menu():
    print("\n" + "=" * 60)
    print("        é€šç”¨å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼ˆæ™ºèƒ½æå–ç‰ˆï¼‰")
    print("=" * 60)
    print("1. æŸ¥çœ‹DOCSæ–‡ä»¶å¤¹ä¸­çš„è®ºæ–‡åˆ—è¡¨")
    print("2. åˆ†ææŒ‡å®šå•ç¯‡è®ºæ–‡ï¼ˆLLM+æ™ºèƒ½FallbackåŒä¿éšœï¼‰")
    print("3. æ‰¹é‡åˆ†ææ‰€æœ‰æœ‰æ•ˆè®ºæ–‡")
    print("4. å¯¼å‡ºå•ç¯‡è®ºæ–‡åˆ†æç»“æœï¼ˆJSONæ ¼å¼ï¼‰")
    print("5. é€€å‡ºç¨‹åº")
    print("=" * 60)


def list_papers_terminal():
    papers = get_all_papers()
    if not papers or papers[0]["status"] == "empty":
        print(f"\nâš ï¸  {papers[0]['paper_name']}")
        return None

    print("\nğŸ“„ DOCSæ–‡ä»¶å¤¹ä¸­çš„æœ‰æ•ˆè®ºæ–‡åˆ—è¡¨ï¼š")
    print("-" * 90)
    print(f"{'ç´¢å¼•':<5} {'è®ºæ–‡æ–‡ä»¶å':<60} {'æ–‡ä»¶ç±»å‹':<10} {'çŠ¶æ€':<10}")
    print("-" * 90)
    for paper in papers:
        if paper["status"] == "valid":
            file_type = "PDF" if paper["file_ext"] == ".pdf" else "å…¶ä»–"
            print(f"{paper['index']:<5} {paper['paper_name'].split('ï¼ˆ')[0]:<60} {file_type:<10} å¯åˆ†æ")
    return papers


def export_analysis_result(result: dict, output_dir: str = "ANALYSIS_OUTPUT"):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    paper_name = result["paper_info"]["paper_name"].split("ï¼ˆ")[0].replace(".", "_").replace("/", "_")
    output_path = os.path.join(output_dir, f"analysis_{paper_name}_{result['extraction_source']}.json")

    export_data = {
        "export_info": {
            "source": result["extraction_source"],
            "time": "2025å¹´ï¼ˆç»ˆç«¯ç‰ˆæ— å®æ—¶æ—¶é—´ï¼‰"
        },
        "paper_info": result["paper_info"],
        "is_yolo_related": result["is_yolo_related"],
        "analysis_result": result["analysis_result"]
    }

    try:
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… ç»“æœå·²å¯¼å‡ºè‡³ï¼š{output_path}")
        return output_path
    except Exception as e:
        print(f"\nâŒ å¯¼å‡ºå¤±è´¥ï¼š{str(e)}")
        return None


def terminal_interaction():
    current_analysis_result = None

    while True:
        print_terminal_menu()
        choice = input("\nè¯·è¾“å…¥æ“ä½œç¼–å·ï¼ˆ1-5ï¼‰ï¼š").strip()

        if choice == "1":
            list_papers_terminal()

        elif choice == "2":
            papers = list_papers_terminal()
            if not papers or papers[0]["status"] == "empty":
                continue

            try:
                paper_idx = int(input("\nè¯·è¾“å…¥è¦åˆ†æçš„è®ºæ–‡ç´¢å¼•ï¼š").strip())
                target_paper = next(p for p in papers if p["status"] == "valid" and p["index"] == paper_idx)
            except (ValueError, StopIteration):
                print("\nâŒ æ— æ•ˆç´¢å¼•ï¼Œè¯·é‡æ–°é€‰æ‹©ï¼")
                continue

            # æ‰§è¡Œåˆ†æ
            result = analyze_single_paper(target_paper)
            if result["status"] == "error":
                print(f"\nâŒ {result['msg']}")
                continue

            # ä¿å­˜å½“å‰ç»“æœï¼ˆç”¨äºå¯¼å‡ºï¼‰
            current_analysis_result = result
            analysis = result["analysis_result"]
            is_yolo = result["is_yolo_related"]
            source = result["extraction_source"]

            # æ ¼å¼åŒ–å±•ç¤ºï¼ˆåˆ†æ¨¡å—ï¼Œæ¸…æ™°æ˜“è¯»ï¼‰
            print(f"\nğŸ“Š åˆ†æå®Œæˆï¼ˆæå–æ¥æºï¼š{source}ï¼‰")
            print("=" * 80)
            print(f"1. è®ºæ–‡æ ‡é¢˜ï¼š{analysis['paper_title']}")
            print(f"2. è®ºæ–‡ç±»å‹ï¼š{'âœ… YOLOç³»åˆ—' if is_yolo else 'ğŸ” éYOLOç³»åˆ—'}")
            print("\n" + "-" * 80)
            print("3. æ ¸å¿ƒåˆ›æ–°ç‚¹")
            print("-" * 80)
            print(f"   çªç ´ç‚¹ï¼š{analysis['innovation_point']['core_innovation']}")
            print(f"   ä»·å€¼ï¼š{analysis['innovation_point']['innovation_value']}")
            print(f"   ä¼ªä»£ç ï¼š{analysis['innovation_point']['pseudo_code']}")
            print("\n" + "-" * 80)
            print("4. æ•°å­¦æ¨å¯¼")
            print("-" * 80)
            print(f"   æ ¸å¿ƒå…¬å¼ï¼š\n{analysis['math_derivation']['key_formulas']}")
            print(f"   æ¨å¯¼æ­¥éª¤ï¼š\n{analysis['math_derivation']['derivation_steps']}")
            print(f"   æ•°å­¦ä¼˜åŠ¿ï¼š{analysis['math_derivation']['math_advantage']}")
            print("\n" + "-" * 80)
            print("5. å¤ç°æ­¥éª¤")
            print("-" * 80)
            print(f"   æ•°æ®é›†ï¼š{analysis['reproduction_steps']['data_prep']}")
            print(f"   ç¯å¢ƒé…ç½®ï¼š{analysis['reproduction_steps']['env_config']}")
            print(f"   ç¡¬ä»¶è¦æ±‚ï¼š{analysis['reproduction_steps']['hardware_req']}")
            print(f"   æ ¸å¿ƒæµç¨‹ï¼š\n{analysis['reproduction_steps']['core_steps']}")
            print(f"   ä»£ç è·å–ï¼š{analysis['reproduction_steps']['code_info']}")
            print("\n" + "-" * 80)
            print("6. å¯¹æ¯”å®éªŒ")
            print("-" * 80)
            print(f"   å¯¹æ¯”æ–¹æ³•ï¼š{analysis['comparison_experiments']['compared_methods']}")
            print(f"   è¯„ä»·æŒ‡æ ‡ï¼š{analysis['comparison_experiments']['evaluation_metrics']}")
            print(f"   æ ¸å¿ƒç»“æœï¼š\n{analysis['comparison_experiments']['key_results']}")
            print(f"   å®éªŒç»“è®ºï¼š{analysis['comparison_experiments']['experiment_conclusion']}")
            print("=" * 80)

        elif choice == "3":
            print("\nâš ï¸  æ‰¹é‡åˆ†æä¸­ï¼Œè¯·å‹¿å…³é—­ç¨‹åº...ï¼ˆæ¯ç¯‡è®ºæ–‡çº¦30ç§’ï¼‰")
            batch_result = json.loads(batch_analyze_papers())

            print("\n" + "=" * 60)
            print("        æ‰¹é‡åˆ†ææ€»ç»“æŠ¥å‘Š")
            print("=" * 60)
            print(f"ğŸ“Š æ•´ä½“ç»Ÿè®¡ï¼š")
            print(f"   - æ€»è®ºæ–‡æ•°ï¼š{batch_result['total_papers']}")
            print(f"   - æœ‰æ•ˆè®ºæ–‡æ•°ï¼š{batch_result['valid_papers_count']}")
            print(f"   - æˆåŠŸåˆ†ææ•°ï¼š{batch_result['analyzed_count']}")
            print(f"   - YOLOç³»åˆ—ï¼š{batch_result['yolo_related_count']}ç¯‡")
            print(f"   - éYOLOç³»åˆ—ï¼š{batch_result['non_yolo_related_count']}ç¯‡")
            print(f"   - åˆ†æçŠ¶æ€ï¼š{batch_result['overall_status']}")

            if batch_result["analyzed_papers"]:
                print(f"\nğŸ“„ åˆ†æè¯¦æƒ…ï¼ˆå‰5ç¯‡ï¼‰ï¼š")
                for i, p in enumerate(batch_result["analyzed_papers"][:5], 1):
                    title = p["analysis_result"]["paper_title"][:50] + "..." if len(
                        p["analysis_result"]["paper_title"]) > 50 else p["analysis_result"]["paper_title"]
                    print(f"   {i}. è®ºæ–‡ï¼š{p['paper_name'].split('ï¼ˆ')[0]}")
                    print(f"      æ ‡é¢˜ï¼š{title}")
                    print(f"      ç±»å‹ï¼š{'YOLO' if p['is_yolo_related'] else 'éYOLO'} | æ¥æºï¼š{p['extraction_source']}")

            if batch_result["error_log"]:
                print(f"\nâŒ é”™è¯¯è®°å½•ï¼ˆå…±{len(batch_result['error_log'])}æ¡ï¼‰ï¼š")
                for idx, err in enumerate(batch_result["error_log"], 1):
                    print(f"   {idx}. {err}")
            print("=" * 60)

        elif choice == "4":
            if not current_analysis_result or current_analysis_result["status"] != "success":
                print("\nâŒ æ— å¯ç”¨åˆ†æç»“æœï¼Œè¯·å…ˆæ‰§è¡Œã€Œ2. åˆ†ææŒ‡å®šå•ç¯‡è®ºæ–‡ã€ï¼")
                continue
            export_analysis_result(current_analysis_result)

        elif choice == "5":
            print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œç¨‹åºå·²é€€å‡ºï¼")
            break

        else:
            print("\nâŒ æ— æ•ˆæ“ä½œç¼–å·ï¼Œè¯·è¾“å…¥1-5ä¹‹é—´çš„æ•°å­—ï¼")


# -------------------------- 6. ç¨‹åºå…¥å£ï¼ˆåŒæ¨¡å¼æ”¯æŒï¼‰ --------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="é€šç”¨å­¦æœ¯è®ºæ–‡åˆ†æå·¥å…·ï¼ˆLLM+æ™ºèƒ½Fallbackï¼‰")
    parser.add_argument("--mode", type=str, default="terminal", choices=["terminal", "web"],
                        help="è¿è¡Œæ¨¡å¼ï¼šterminalï¼ˆæœ¬åœ°ç»ˆç«¯ï¼‰/ webï¼ˆWebæœåŠ¡ï¼‰")
    args = parser.parse_args()

    try:
        if args.mode == "terminal":
            print("ğŸ‰ æœ¬åœ°ç»ˆç«¯æ¨¡å¼å¯åŠ¨æˆåŠŸï¼æ”¯æŒLLMåˆ†æ+æ™ºèƒ½Fallbackï¼ˆæ— LLMä¹Ÿèƒ½æå–ï¼‰")
            terminal_interaction()

        elif args.mode == "web":
            batch_agent = ReactAgent(
                llm=OnlineChatModule(
                    source='sensenova',
                    stream=False,
                    model='your_model',
                    api_key='your_api'
                ),
                tools=['batch_analyze_papers'],
                prompt=f"""
                ã€è§’è‰²ã€‘é€šç”¨å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼Œéœ€å±•ç¤ºå®Œæ•´åˆ†æç»“æœã€‚
                ã€å±•ç¤ºè¦æ±‚ã€‘
                1. å…ˆæ˜¾ç¤ºæ‰¹é‡æ€»ç»“ï¼ˆæ€»è®ºæ–‡æ•°/YOLOæ•°/éYOLOæ•°/é”™è¯¯æ•°ï¼‰ï¼›
                2. æ¯ç¯‡è®ºæ–‡æŒ‰ã€Œæ ‡é¢˜â†’ç±»å‹â†’åˆ›æ–°ç‚¹â†’æ•°å­¦â†’å¤ç°â†’å¯¹æ¯”ã€é¡ºåºå±•ç¤ºï¼›
                3. æ ‡æ³¨æå–æ¥æºï¼ˆLLM/æ™ºèƒ½æå–ï¼‰ï¼›
                4. éYOLOè®ºæ–‡ä¼ªä»£ç å›ºå®šï¼Œæ— éœ€ä¿®æ”¹ã€‚
                """,
                stream=False
            )
            LOG.info("é€šç”¨å­¦æœ¯è®ºæ–‡åˆ†æAgentåˆå§‹åŒ–å®Œæˆ")

            web_app = WebModule(
                batch_agent,
                port=8847,
                title="é€šç”¨å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ï¼ˆLLM+æ™ºèƒ½Fallbackï¼‰"
            )
            LOG.info("WebæœåŠ¡å¯åŠ¨æˆåŠŸï¼Œè®¿é—®åœ°å€ï¼šhttp://localhost:8847")
            web_app.start().wait()

    except Exception as e:
        LOG.error(f"ç¨‹åºå¯åŠ¨å¤±è´¥ï¼š{str(e)}")
        print(
            f"å¯åŠ¨é”™è¯¯ï¼š{str(e)}\nè¯·æ£€æŸ¥ï¼š\n1. APIå¯†é’¥æ˜¯å¦æ­£ç¡®ï¼›\n2. ä¾èµ–åº“æ˜¯å¦å®‰è£…ï¼ˆpip install PyPDF2 python-docx lazyllm argparseï¼‰ï¼›\n3. ç«¯å£8847æ˜¯å¦è¢«å ç”¨ï¼ˆWebæ¨¡å¼ï¼‰ï¼›\n4. è®ºæ–‡æ˜¯å¦ä¸ºæ”¯æŒæ ¼å¼")